# _target_: models.sequence.SequenceModel
_name_: model
layer:
  - _name_: localattn
    n_heads: ${model.lg_local_heads}
    chunk_size: ${model.lg_local_chunk_size}
    act_fun: relu
    norm_type: ${model.norm}
    use_lrpe: true
    use_softmax: ${model.use_softmax}
  - _name_: linearattn
    n_heads: ${model.lg_linear_heads}
    act_fun: ${model.act_fun}
    norm_type: ${model.norm}
    use_lrpe: true
  - _name_: ff
    expand: ${model.expand_ratio_ffn}
    dropout: null
    transposed: False

lg_local_heads: 8
lg_local_chunk_size: 64
lg_linear_heads: 8
use_softmax: true
act_fun: 1+elu
expand_ratio_ffn: 1

flash_linear_s: 0
flash_linear_max_position_embeddings: 0
flash_max_position_embed: 0
flash_s: 0
ls_attn_heads: 0
ls_attn_window_size: 0
ls_attn_max_seq_len: 0
performer_heads: 0
performer_approx_attn_dim: 0
cosformer_heads: 0 
cosformer_max_length: 0
linformer_max_seq_len: 0
reformer_max_seq_len: 0

residual: R
dropout: 0.1
encoder:
  _name_: position
  dropout: 0.1
# init:
#   init: normal  # Parameter initializer to use
#   init_range: 0.1  # Parameters initialized by U(-init_range, init_range)
#   init_std: 0.02  # Parameters initialized by N(0, init_std)