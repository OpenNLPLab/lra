CONFIG
├── train
│   └── seed: 2222                                                                                 
│       interval: step                                                                             
│       monitor: val/accuracy                                                                      
│       mode: max                                                                                  
│       ema: 0.0                                                                                   
│       test: true                                                                                 
│       debug: false                                                                               
│       ignore_warnings: false                                                                     
│       state:                                                                                     
│         mode: null                                                                               
│         chunk_len: null                                                                          
│         overlap_len: null                                                                        
│         n_context: 0                                                                             
│         n_context_eval: 0                                                                        
│       sweep: null                                                                                
│       group: null                                                                                
│       benchmark_step: false                                                                      
│       benchmark_step_k: 1                                                                        
│       benchmark_step_T: 1                                                                        
│       checkpoint_path: null                                                                      
│       visualizer: filters                                                                        
│       disable_dataset: false                                                                     
│                                                                                                  
├── wandb
│   └── None                                                                                       
├── trainer
│   └── gpus: 1                                                                                    
│       accumulate_grad_batches: 1                                                                 
│       max_epochs: 200                                                                            
│       gradient_clip_val: 0.0                                                                     
│       log_every_n_steps: 10                                                                      
│       limit_train_batches: 1.0                                                                   
│       limit_val_batches: 1.0                                                                     
│       weights_summary: top                                                                       
│       progress_bar_refresh_rate: 1                                                               
│       track_grad_norm: -1                                                                        
│       resume_from_checkpoint: null                                                               
│                                                                                                  
├── loader
│   └── batch_size: 16                                                                             
│       num_workers: 4                                                                             
│       pin_memory: true                                                                           
│       drop_last: true                                                                            
│       train_resolution: 1                                                                        
│       eval_resolutions:                                                                          
│       - 1                                                                                        
│                                                                                                  
├── dataset
│   └── _name_: cifar                                                                              
│       permute: null                                                                              
│       grayscale: true                                                                            
│       tokenize: false                                                                            
│       augment: false                                                                             
│       cutout: false                                                                              
│       random_erasing: false                                                                      
│       val_split: 0.1                                                                             
│       seed: 42                                                                                   
│                                                                                                  
├── task
│   └── _name_: base                                                                               
│       loss: cross_entropy                                                                        
│       metrics:                                                                                   
│       - accuracy                                                                                 
│       torchmetrics: null                                                                         
│                                                                                                  
├── optimizer
│   └── _name_: adamw                                                                              
│       lr: 0.007                                                                                  
│       weight_decay: 0.001                                                                        
│                                                                                                  
├── scheduler
│   └── _name_: cosine_warmup                                                                      
│       num_warmup_steps: 30000                                                                    
│       num_training_steps: 50000                                                                  
│                                                                                                  
├── encoder
│   └── linear                                                                                     
├── decoder
│   └── _name_: sequence                                                                           
│       mode: pool                                                                                 
│                                                                                                  
├── model
│   └── _name_: model                                                                              
│       layer:                                                                                     
│       - _name_: gtu                                                                              
│         n_heads: 1                                                                               
│         expand_ratio: 2                                                                          
│         use_decay: true                                                                          
│         gamma: 0.7                                                                               
│         rpe_embedding: 16                                                                        
│         rpe_layers: 2                                                                            
│       - _name_: glu                                                                              
│         act_fun: swish                                                                           
│         glu_expand_ratio: 3                                                                      
│       rpe_layers: 2                                                                              
│       gtu_head: 1                                                                                
│       gtu_use_decay: true                                                                        
│       gtu_gamma: 0.7                                                                             
│       gtu_rpe_dim: 16                                                                            
│       expand_ratio_gtu: 2                                                                        
│       expand_ratio_glu: 3                                                                        
│       flash_s: 0                                                                                 
│       flash_max_position_embed: 0                                                                
│       flash_linear_s: 0                                                                          
│       flash_linear_max_position_embeddings: 0                                                    
│       lg_local_heads: 0                                                                          
│       lg_linear_heads: 0                                                                         
│       lg_local_chunk_size: 0                                                                     
│       ls_attn_heads: 0                                                                           
│       ls_attn_window_size: 0                                                                     
│       ls_attn_max_seq_len: 0                                                                     
│       performer_heads: 0                                                                         
│       performer_approx_attn_dim: 0                                                               
│       use_softmax: true                                                                          
│       act_fun: 1+elu                                                                             
│       cosformer_heads: 0                                                                         
│       cosformer_max_length: 0                                                                    
│       linformer_max_seq_len: 0                                                                   
│       residual: R                                                                                
│       dropout: 0.1                                                                               
│       encoder:                                                                                   
│         _name_: position                                                                         
│         dropout: 0.1                                                                             
│       n_layers: 2                                                                                
│       d_model: 32                                                                                
│       prenorm: true                                                                              
│       norm: batch                                                                                
│                                                                                                  
└── callbacks
    └── learning_rate_monitor:                                                                     
          logging_interval: step                                                                   
        timer:                                                                                     
          step: true                                                                               
          inter_step: false                                                                        
          epoch: true                                                                              
          val: true                                                                                
        params:                                                                                    
          total: true                                                                              
          trainable: true                                                                          
          fixed: true                                                                              
        model_checkpoint:                                                                          
          monitor: val/accuracy                                                                    
          mode: max                                                                                
          save_top_k: 1                                                                            
          save_last: true                                                                          
          dirpath: checkpoints/                                                                    
          filename: val/accuracy                                                                   
          auto_insert_metric_name: false                                                           
          verbose: true                                                                            
                                                                                                   
Global seed set to 2222
[2022-12-21 23:59:11,065][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2022-12-21 23:59:11,067][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2022-12-21 23:59:11,068][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Files already downloaded and verified
[2022-12-21 23:59:12,128][print_config][INFO] - self.expand_ratio 2
[2022-12-21 23:59:12,128][print_config][INFO] - self.resi_param False
[2022-12-21 23:59:12,129][print_config][INFO] - silu
[2022-12-21 23:59:12,129][print_config][INFO] - act_fun silu
[2022-12-21 23:59:12,129][print_config][INFO] - causal False
[2022-12-21 23:59:12,130][print_config][INFO] - none
[2022-12-21 23:59:12,130][print_config][INFO] - self.num_heads 1
[2022-12-21 23:59:12,130][print_config][INFO] - self.max_l 512
[2022-12-21 23:59:12,131][print_config][INFO] - self.use_exp False
[2022-12-21 23:59:12,131][print_config][INFO] - self.use_neg_exp False
[2022-12-21 23:59:12,131][print_config][INFO] - self.use_decay True
[2022-12-21 23:59:12,131][print_config][INFO] - self.use_multi_decay False
[2022-12-21 23:59:12,131][print_config][INFO] - self.rpe_embedding 16
[2022-12-21 23:59:12,131][print_config][INFO] - self.rpe_act relu
[2022-12-21 23:59:12,131][print_config][INFO] - self.rpe_use_pad True
[2022-12-21 23:59:12,131][print_config][INFO] - self.normalize False
[2022-12-21 23:59:12,131][print_config][INFO] - self.par_type 1
[2022-12-21 23:59:12,131][print_config][INFO] - self.rpe_type 1
[2022-12-21 23:59:12,131][print_config][INFO] - self.residual False
[2022-12-21 23:59:12,131][print_config][INFO] - self.l 1
[2022-12-21 23:59:12,131][print_config][INFO] - self.transform_type 1
[2022-12-21 23:59:12,131][print_config][INFO] - self.gamma 0.7
[2022-12-21 23:59:12,131][print_config][INFO] - bias True
[2022-12-21 23:59:12,132][print_config][INFO] - tno_type 1
[2022-12-21 23:59:12,132][print_config][INFO] - rpe_layers 2
[2022-12-21 23:59:12,132][print_config][INFO] - here! layer norm
[2022-12-21 23:59:12,132][print_config][INFO] - use_norm False
[2022-12-21 23:59:12,132][print_config][INFO] - norm_type layernorm
[2022-12-21 23:59:12,132][print_config][INFO] - self.token_shift_type -1
[2022-12-21 23:59:12,133][print_config][INFO] - swish
[2022-12-21 23:59:12,133][print_config][INFO] - None
[2022-12-21 23:59:12,133][print_config][INFO] - act_fun swish
[2022-12-21 23:59:12,133][print_config][INFO] - dropout 0.1
[2022-12-21 23:59:12,133][print_config][INFO] - final None
[2022-12-21 23:59:12,134][print_config][INFO] - self.expand_ratio 2
[2022-12-21 23:59:12,134][print_config][INFO] - self.resi_param False
[2022-12-21 23:59:12,135][print_config][INFO] - silu
[2022-12-21 23:59:12,135][print_config][INFO] - act_fun silu
[2022-12-21 23:59:12,135][print_config][INFO] - causal False
[2022-12-21 23:59:12,136][print_config][INFO] - none
[2022-12-21 23:59:12,136][print_config][INFO] - self.num_heads 1
[2022-12-21 23:59:12,136][print_config][INFO] - self.max_l 512
[2022-12-21 23:59:12,136][print_config][INFO] - self.use_exp False
[2022-12-21 23:59:12,136][print_config][INFO] - self.use_neg_exp False
[2022-12-21 23:59:12,136][print_config][INFO] - self.use_decay True
[2022-12-21 23:59:12,136][print_config][INFO] - self.use_multi_decay False
[2022-12-21 23:59:12,137][print_config][INFO] - self.rpe_embedding 16
[2022-12-21 23:59:12,137][print_config][INFO] - self.rpe_act relu
[2022-12-21 23:59:12,137][print_config][INFO] - self.rpe_use_pad True
[2022-12-21 23:59:12,137][print_config][INFO] - self.normalize False
[2022-12-21 23:59:12,137][print_config][INFO] - self.par_type 1
[2022-12-21 23:59:12,137][print_config][INFO] - self.rpe_type 1
[2022-12-21 23:59:12,137][print_config][INFO] - self.residual False
[2022-12-21 23:59:12,137][print_config][INFO] - self.l 1
[2022-12-21 23:59:12,137][print_config][INFO] - self.transform_type 1
[2022-12-21 23:59:12,137][print_config][INFO] - self.gamma 0.7
[2022-12-21 23:59:12,137][print_config][INFO] - bias True
[2022-12-21 23:59:12,137][print_config][INFO] - tno_type 1
[2022-12-21 23:59:12,137][print_config][INFO] - rpe_layers 2
[2022-12-21 23:59:12,137][print_config][INFO] - here! layer norm
[2022-12-21 23:59:12,138][print_config][INFO] - use_norm False
[2022-12-21 23:59:12,138][print_config][INFO] - norm_type layernorm
[2022-12-21 23:59:12,138][print_config][INFO] - self.token_shift_type -1
[2022-12-21 23:59:12,141][print_config][INFO] - swish
[2022-12-21 23:59:12,141][print_config][INFO] - None
[2022-12-21 23:59:12,141][print_config][INFO] - act_fun swish
[2022-12-21 23:59:12,141][print_config][INFO] - dropout 0.1
[2022-12-21 23:59:12,142][print_config][INFO] - final None
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SequenceModel(
  (drop): Identity()
  (layers): ModuleList(
    (0): SequenceResidualBlock(
      (layer): Gtu(
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (pos1): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos2): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos3): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
        (pre_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): SequenceResidualBlock(
      (layer): Gtu(
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (pos1): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos2): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos3): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
        (pre_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (3): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (norm): Normalization(
    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
[2022-12-21 23:59:14,952][__main__][INFO] - Optimizer group 0 | 64 tensors

  | Name    | Type            | Params
--------------------------------------------
0 | model   | SequenceModel   | 35.5 K
1 | encoder | Sequential      | 64    
2 | decoder | SequenceDecoder | 330   
--------------------------------------------
35.7 K    Trainable params
128       Non-trainable params
35.9 K    Total params
0.143     Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/4 [00:00<?, ?it/s]Validation sanity check:  25%|██▌       | 1/4 [00:00<00:00,  4.98it/s]Validation sanity check:  75%|███████▌  | 3/4 [00:00<00:00,  8.29it/s]                                                                      Global seed set to 2222
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/3749 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3749 [00:00<?, ?it/s] Epoch 0:   0%|          | 1/3749 [00:00<10:34,  5.91it/s]Epoch 0:   0%|          | 1/3749 [00:00<10:35,  5.90it/s, loss=2.29]Epoch 0:   0%|          | 2/3749 [00:00<05:53, 10.60it/s, loss=2.28]Epoch 0:   0%|          | 3/3749 [00:00<04:17, 14.54it/s, loss=2.29]Epoch 0:   0%|          | 4/3749 [00:00<03:30, 17.79it/s, loss=2.28]Epoch 0:   0%|          | 5/3749 [00:00<03:04, 20.35it/s, loss=2.29]Epoch 0:   0%|          | 6/3749 [00:00<02:44, 22.76it/s, loss=2.29]Epoch 0:   0%|          | 7/3749 [00:00<02:32, 24.52it/s, loss=2.29]Epoch 0:   0%|          | 7/3749 [00:00<02:32, 24.48it/s, loss=2.29]Epoch 0:   0%|          | 8/3749 [00:00<02:22, 26.18it/s, loss=2.3] Epoch 0:   0%|          | 9/3749 [00:00<02:16, 27.44it/s, loss=2.3]Epoch 0:   0%|          | 10/3749 [00:00<02:09, 28.87it/s, loss=2.3]Epoch 0:   0%|          | 11/3749 [00:00<02:04, 29.97it/s, loss=2.3]Epoch 0:   0%|          | 12/3749 [00:00<02:00, 30.95it/s, loss=2.3]Epoch 0:   0%|          | 13/3749 [00:00<01:56, 32.12it/s, loss=2.3]Epoch 0:   0%|          | 13/3749 [00:00<01:56, 32.09it/s, loss=2.3]Epoch 0:   0%|          | 14/3749 [00:00<01:53, 32.89it/s, loss=2.3]Epoch 0:   0%|          | 15/3749 [00:00<01:50, 33.78it/s, loss=2.3]Epoch 0:   0%|          | 16/3749 [00:00<01:47, 34.66it/s, loss=2.3]Epoch 0:   0%|          | 17/3749 [00:00<01:45, 35.28it/s, loss=2.3]Epoch 0:   0%|          | 18/3749 [00:00<01:43, 36.05it/s, loss=2.3]Epoch 0:   1%|          | 19/3749 [00:00<01:42, 36.47it/s, loss=2.3]Epoch 0:   1%|          | 19/3749 [00:00<01:42, 36.44it/s, loss=2.3]Epoch 0:   1%|          | 20/3749 [00:00<01:40, 37.10it/s, loss=2.3]Epoch 0:   1%|          | 21/3749 [00:00<01:38, 37.68it/s, loss=2.3]Epoch 0:   1%|          | 22/3749 [00:00<01:37, 38.12it/s, loss=2.3]Epoch 0:   1%|          | 23/3749 [00:00<01:36, 38.66it/s, loss=2.3]Epoch 0:   1%|          | 24/3749 [00:00<01:38, 37.98it/s, loss=2.3]Epoch 0:   1%|          | 25/3749 [00:00<01:36, 38.42it/s, loss=2.3]Epoch 0:   1%|          | 25/3749 [00:00<01:37, 38.39it/s, loss=2.3]Epoch 0:   1%|          | 26/3749 [00:00<01:37, 38.36it/s, loss=2.3]Epoch 0:   1%|          | 27/3749 [00:00<01:36, 38.55it/s, loss=2.3]Epoch 0:   1%|          | 28/3749 [00:00<01:35, 38.85it/s, loss=2.3]Epoch 0:   1%|          | 29/3749 [00:00<01:34, 39.30it/s, loss=2.3]Epoch 0:   1%|          | 30/3749 [00:00<01:34, 39.55it/s, loss=2.3]Epoch 0:   1%|          | 31/3749 [00:00<01:33, 39.83it/s, loss=2.3]Epoch 0:   1%|          | 31/3749 [00:00<01:33, 39.81it/s, loss=2.3]Epoch 0:   1%|          | 32/3749 [00:00<01:32, 40.04it/s, loss=2.3]Epoch 0:   1%|          | 33/3749 [00:00<01:32, 40.17it/s, loss=2.3]Epoch 0:   1%|          | 34/3749 [00:00<01:31, 40.53it/s, loss=2.3]Epoch 0:   1%|          | 35/3749 [00:00<01:31, 40.77it/s, loss=2.3]Epoch 0:   1%|          | 36/3749 [00:00<01:30, 41.08it/s, loss=2.3]Epoch 0:   1%|          | 37/3749 [00:00<01:29, 41.36it/s, loss=2.3]Epoch 0:   1%|          | 37/3749 [00:00<01:29, 41.34it/s, loss=2.3]Epoch 0:   1%|          | 38/3749 [00:00<01:29, 41.54it/s, loss=2.31]Epoch 0:   1%|          | 39/3749 [00:00<01:29, 41.58it/s, loss=2.31]Epoch 0:   1%|          | 40/3749 [00:00<01:29, 41.50it/s, loss=2.31]Epoch 0:   1%|          | 41/3749 [00:00<01:29, 41.55it/s, loss=2.3] Epoch 0:   1%|          | 42/3749 [00:01<01:29, 41.59it/s, loss=2.31]Epoch 0:   1%|          | 43/3749 [00:01<01:28, 41.77it/s, loss=2.31]Epoch 0:   1%|          | 43/3749 [00:01<01:28, 41.76it/s, loss=2.3] Epoch 0:   1%|          | 44/3749 [00:01<01:28, 41.82it/s, loss=2.3]Epoch 0:   1%|          | 45/3749 [00:01<01:28, 42.03it/s, loss=2.3]Epoch 0:   1%|          | 46/3749 [00:01<01:27, 42.22it/s, loss=2.3]Epoch 0:   1%|▏         | 47/3749 [00:01<01:27, 42.35it/s, loss=2.3]Epoch 0:   1%|▏         | 48/3749 [00:01<01:26, 42.57it/s, loss=2.29]Epoch 0:   1%|▏         | 49/3749 [00:01<01:26, 42.68it/s, loss=2.29]Epoch 0:   1%|▏         | 49/3749 [00:01<01:26, 42.67it/s, loss=2.29]Epoch 0:   1%|▏         | 50/3749 [00:01<01:26, 42.87it/s, loss=2.3] Epoch 0:   1%|▏         | 51/3749 [00:01<01:26, 43.00it/s, loss=2.3]Epoch 0:   1%|▏         | 52/3749 [00:01<01:25, 43.05it/s, loss=2.3]Epoch 0:   1%|▏         | 53/3749 [00:01<01:25, 43.21it/s, loss=2.3]Epoch 0:   1%|▏         | 54/3749 [00:01<01:25, 43.20it/s, loss=2.3]Epoch 0:   1%|▏         | 55/3749 [00:01<01:25, 43.37it/s, loss=2.3]Epoch 0:   1%|▏         | 55/3749 [00:01<01:25, 43.35it/s, loss=2.3]Epoch 0:   1%|▏         | 56/3749 [00:01<01:24, 43.51it/s, loss=2.3]Epoch 0:   2%|▏         | 57/3749 [00:01<01:25, 43.39it/s, loss=2.3]Epoch 0:   2%|▏         | 58/3749 [00:01<01:25, 43.42it/s, loss=2.3]Epoch 0:   2%|▏         | 59/3749 [00:01<01:24, 43.49it/s, loss=2.31]Epoch 0:   2%|▏         | 60/3749 [00:01<01:24, 43.63it/s, loss=2.3] Epoch 0:   2%|▏         | 61/3749 [00:01<01:24, 43.73it/s, loss=2.3]Epoch 0:   2%|▏         | 61/3749 [00:01<01:24, 43.72it/s, loss=2.3]Epoch 0:   2%|▏         | 62/3749 [00:01<01:24, 43.87it/s, loss=2.3]Epoch 0:   2%|▏         | 63/3749 [00:01<01:23, 44.02it/s, loss=2.3]Epoch 0:   2%|▏         | 64/3749 [00:01<01:23, 43.88it/s, loss=2.31]Epoch 0:   2%|▏         | 65/3749 [00:01<01:23, 43.87it/s, loss=2.3] Epoch 0:   2%|▏         | 66/3749 [00:01<01:23, 43.96it/s, loss=2.3]Epoch 0:   2%|▏         | 67/3749 [00:01<01:23, 44.08it/s, loss=2.3]Epoch 0:   2%|▏         | 67/3749 [00:01<01:23, 44.07it/s, loss=2.31]Epoch 0:   2%|▏         | 68/3749 [00:01<01:23, 44.13it/s, loss=2.31]Epoch 0:   2%|▏         | 69/3749 [00:01<01:23, 44.26it/s, loss=2.31]Epoch 0:   2%|▏         | 70/3749 [00:01<01:22, 44.38it/s, loss=2.31]Epoch 0:   2%|▏         | 71/3749 [00:01<01:22, 44.44it/s, loss=2.31]Epoch 0:   2%|▏         | 72/3749 [00:01<01:22, 44.58it/s, loss=2.3] Epoch 0:   2%|▏         | 73/3749 [00:01<01:22, 44.60it/s, loss=2.3]Epoch 0:   2%|▏         | 73/3749 [00:01<01:22, 44.59it/s, loss=2.3]Epoch 0:   2%|▏         | 74/3749 [00:01<01:22, 44.69it/s, loss=2.3]Epoch 0:   2%|▏         | 75/3749 [00:01<01:22, 44.70it/s, loss=2.3]Epoch 0:   2%|▏         | 76/3749 [00:01<01:22, 44.79it/s, loss=2.3]Epoch 0:   2%|▏         | 77/3749 [00:01<01:21, 44.87it/s, loss=2.3]Epoch 0:   2%|▏         | 78/3749 [00:01<01:21, 44.89it/s, loss=2.3]Epoch 0:   2%|▏         | 79/3749 [00:01<01:21, 45.01it/s, loss=2.3]Epoch 0:   2%|▏         | 79/3749 [00:01<01:21, 45.00it/s, loss=2.3]Epoch 0:   2%|▏         | 80/3749 [00:01<01:21, 45.01it/s, loss=2.3]Epoch 0:   2%|▏         | 81/3749 [00:01<01:21, 45.09it/s, loss=2.3]Epoch 0:   2%|▏         | 82/3749 [00:01<01:21, 45.16it/s, loss=2.3]Epoch 0:   2%|▏         | 83/3749 [00:01<01:21, 45.16it/s, loss=2.3]Epoch 0:   2%|▏         | 84/3749 [00:01<01:20, 45.28it/s, loss=2.3]Epoch 0:   2%|▏         | 85/3749 [00:01<01:20, 45.29it/s, loss=2.3]Epoch 0:   2%|▏         | 85/3749 [00:01<01:20, 45.29it/s, loss=2.31]Epoch 0:   2%|▏         | 86/3749 [00:01<01:20, 45.38it/s, loss=2.31]Epoch 0:   2%|▏         | 87/3749 [00:01<01:20, 45.48it/s, loss=2.31]Epoch 0:   2%|▏         | 88/3749 [00:01<01:20, 45.52it/s, loss=2.31]Epoch 0:   2%|▏         | 89/3749 [00:01<01:20, 45.62it/s, loss=2.31]Epoch 0:   2%|▏         | 90/3749 [00:01<01:20, 45.41it/s, loss=2.3] Epoch 0:   2%|▏         | 91/3749 [00:02<01:20, 45.31it/s, loss=2.3]Epoch 0:   2%|▏         | 91/3749 [00:02<01:20, 45.29it/s, loss=2.3]Epoch 0:   2%|▏         | 92/3749 [00:02<01:21, 45.13it/s, loss=2.3]Epoch 0:   2%|▏         | 93/3749 [00:02<01:21, 45.00it/s, loss=2.3]Epoch 0:   3%|▎         | 94/3749 [00:02<01:21, 44.88it/s, loss=2.3]Epoch 0:   3%|▎         | 95/3749 [00:02<01:21, 44.84it/s, loss=2.3]Epoch 0:   3%|▎         | 96/3749 [00:02<01:21, 44.82it/s, loss=2.3]Epoch 0:   3%|▎         | 97/3749 [00:02<01:21, 44.90it/s, loss=2.3]Epoch 0:   3%|▎         | 97/3749 [00:02<01:21, 44.90it/s, loss=2.3]Epoch 0:   3%|▎         | 98/3749 [00:02<01:21, 44.87it/s, loss=2.3]Epoch 0:   3%|▎         | 99/3749 [00:02<01:21, 44.94it/s, loss=2.3]Epoch 0:   3%|▎         | 100/3749 [00:02<01:21, 44.98it/s, loss=2.3]Epoch 0:   3%|▎         | 101/3749 [00:02<01:21, 45.04it/s, loss=2.29]Epoch 0:   3%|▎         | 102/3749 [00:02<01:20, 45.11it/s, loss=2.29]Epoch 0:   3%|▎         | 103/3749 [00:02<01:20, 45.14it/s, loss=2.29]Epoch 0:   3%|▎         | 103/3749 [00:02<01:20, 45.13it/s, loss=2.29]Epoch 0:   3%|▎         | 104/3749 [00:02<01:20, 45.22it/s, loss=2.29]Epoch 0:   3%|▎         | 105/3749 [00:02<01:20, 45.21it/s, loss=2.29]Epoch 0:   3%|▎         | 106/3749 [00:02<01:20, 45.29it/s, loss=2.29]Epoch 0:   3%|▎         | 107/3749 [00:02<01:20, 45.36it/s, loss=2.29]Epoch 0:   3%|▎         | 108/3749 [00:02<01:20, 45.42it/s, loss=2.29]Epoch 0:   3%|▎         | 109/3749 [00:02<01:20, 45.49it/s, loss=2.29]Epoch 0:   3%|▎         | 109/3749 [00:02<01:20, 45.48it/s, loss=2.29]Epoch 0:   3%|▎         | 110/3749 [00:02<01:20, 45.47it/s, loss=2.28]Epoch 0:   3%|▎         | 111/3749 [00:02<01:19, 45.55it/s, loss=2.28]Epoch 0:   3%|▎         | 112/3749 [00:02<01:19, 45.62it/s, loss=2.28]Epoch 0:   3%|▎         | 113/3749 [00:02<01:19, 45.65it/s, loss=2.28]Epoch 0:   3%|▎         | 114/3749 [00:02<01:19, 45.71it/s, loss=2.28]Epoch 0:   3%|▎         | 115/3749 [00:02<01:19, 45.70it/s, loss=2.28]Epoch 0:   3%|▎         | 115/3749 [00:02<01:19, 45.70it/s, loss=2.29]Epoch 0:   3%|▎         | 116/3749 [00:02<01:19, 45.77it/s, loss=2.28]Epoch 0:   3%|▎         | 117/3749 [00:02<01:19, 45.84it/s, loss=2.29]Epoch 0:   3%|▎         | 118/3749 [00:02<01:19, 45.85it/s, loss=2.29]Epoch 0:   3%|▎         | 119/3749 [00:02<01:19, 45.92it/s, loss=2.29]Epoch 0:   3%|▎         | 120/3749 [00:02<01:18, 45.96it/s, loss=2.29]Epoch 0:   3%|▎         | 121/3749 [00:02<01:18, 46.01it/s, loss=2.29]Epoch 0:   3%|▎         | 121/3749 [00:02<01:18, 46.01it/s, loss=2.29]Epoch 0:   3%|▎         | 122/3749 [00:02<01:18, 46.07it/s, loss=2.29]Epoch 0:   3%|▎         | 123/3749 [00:02<01:18, 46.07it/s, loss=2.29]Epoch 0:   3%|▎         | 124/3749 [00:02<01:18, 46.14it/s, loss=2.29]Epoch 0:   3%|▎         | 125/3749 [00:02<01:18, 46.17it/s, loss=2.28]Epoch 0:   3%|▎         | 126/3749 [00:02<01:18, 46.23it/s, loss=2.29]Epoch 0:   3%|▎         | 127/3749 [00:02<01:18, 46.28it/s, loss=2.29]Epoch 0:   3%|▎         | 127/3749 [00:02<01:18, 46.28it/s, loss=2.29]Epoch 0:   3%|▎         | 128/3749 [00:02<01:18, 46.30it/s, loss=2.29]Epoch 0:   3%|▎         | 129/3749 [00:02<01:18, 46.36it/s, loss=2.29]Epoch 0:   3%|▎         | 130/3749 [00:02<01:18, 46.37it/s, loss=2.29]Epoch 0:   3%|▎         | 131/3749 [00:02<01:17, 46.42it/s, loss=2.29]Epoch 0:   4%|▎         | 132/3749 [00:02<01:17, 46.46it/s, loss=2.29]Epoch 0:   4%|▎         | 133/3749 [00:02<01:17, 46.51it/s, loss=2.29]Epoch 0:   4%|▎         | 133/3749 [00:02<01:17, 46.50it/s, loss=2.29]Epoch 0:   4%|▎         | 134/3749 [00:02<01:17, 46.55it/s, loss=2.29]Epoch 0:   4%|▎         | 135/3749 [00:02<01:17, 46.57it/s, loss=2.29]Epoch 0:   4%|▎         | 136/3749 [00:02<01:17, 46.62it/s, loss=2.29]Epoch 0:   4%|▎         | 137/3749 [00:02<01:17, 46.65it/s, loss=2.29]Epoch 0:   4%|▎         | 138/3749 [00:02<01:17, 46.71it/s, loss=2.29]Epoch 0:   4%|▎         | 139/3749 [00:02<01:17, 46.71it/s, loss=2.29]Epoch 0:   4%|▎         | 139/3749 [00:02<01:17, 46.71it/s, loss=2.29]Epoch 0:   4%|▎         | 140/3749 [00:02<01:17, 46.72it/s, loss=2.29]Epoch 0:   4%|▍         | 141/3749 [00:03<01:17, 46.78it/s, loss=2.29]Epoch 0:   4%|▍         | 142/3749 [00:03<01:17, 46.68it/s, loss=2.29]Epoch 0:   4%|▍         | 143/3749 [00:03<01:17, 46.73it/s, loss=2.3] Epoch 0:   4%|▍         | 144/3749 [00:03<01:17, 46.67it/s, loss=2.3]Epoch 0:   4%|▍         | 145/3749 [00:03<01:17, 46.70it/s, loss=2.3]Epoch 0:   4%|▍         | 145/3749 [00:03<01:17, 46.69it/s, loss=2.3]Epoch 0:   4%|▍         | 146/3749 [00:03<01:17, 46.73it/s, loss=2.3]Epoch 0:   4%|▍         | 147/3749 [00:03<01:17, 46.70it/s, loss=2.3]Epoch 0:   4%|▍         | 148/3749 [00:03<01:17, 46.71it/s, loss=2.3]Epoch 0:   4%|▍         | 149/3749 [00:03<01:17, 46.69it/s, loss=2.3]Epoch 0:   4%|▍         | 150/3749 [00:03<01:17, 46.63it/s, loss=2.3]Epoch 0:   4%|▍         | 151/3749 [00:03<01:17, 46.57it/s, loss=2.3]Epoch 0:   4%|▍         | 151/3749 [00:03<01:17, 46.57it/s, loss=2.3]Epoch 0:   4%|▍         | 152/3749 [00:03<01:17, 46.60it/s, loss=2.3]Epoch 0:   4%|▍         | 153/3749 [00:03<01:17, 46.60it/s, loss=2.31]Epoch 0:   4%|▍         | 154/3749 [00:03<01:17, 46.65it/s, loss=2.3] Epoch 0:   4%|▍         | 155/3749 [00:03<01:17, 46.64it/s, loss=2.3]Epoch 0:   4%|▍         | 156/3749 [00:03<01:17, 46.66it/s, loss=2.3]Epoch 0:   4%|▍         | 157/3749 [00:03<01:17, 46.49it/s, loss=2.3]Epoch 0:   4%|▍         | 157/3749 [00:03<01:17, 46.48it/s, loss=2.31]Epoch 0:   4%|▍         | 158/3749 [00:03<01:17, 46.51it/s, loss=2.31]Epoch 0:   4%|▍         | 159/3749 [00:03<01:17, 46.56it/s, loss=2.3] Epoch 0:   4%|▍         | 160/3749 [00:03<01:17, 46.55it/s, loss=2.3]Epoch 0:   4%|▍         | 161/3749 [00:03<01:16, 46.60it/s, loss=2.3]Epoch 0:   4%|▍         | 162/3749 [00:03<01:16, 46.62it/s, loss=2.29]Epoch 0:   4%|▍         | 163/3749 [00:03<01:16, 46.66it/s, loss=2.29]Epoch 0:   4%|▍         | 163/3749 [00:03<01:16, 46.65it/s, loss=2.3] Epoch 0:   4%|▍         | 164/3749 [00:03<01:16, 46.69it/s, loss=2.3]Epoch 0:   4%|▍         | 165/3749 [00:03<01:16, 46.69it/s, loss=2.3]Epoch 0:   4%|▍         | 166/3749 [00:03<01:16, 46.72it/s, loss=2.29]Epoch 0:   4%|▍         | 167/3749 [00:03<01:16, 46.76it/s, loss=2.3] Epoch 0:   4%|▍         | 168/3749 [00:03<01:16, 46.69it/s, loss=2.29]Epoch 0:   5%|▍         | 169/3749 [00:03<01:16, 46.67it/s, loss=2.29]Epoch 0:   5%|▍         | 169/3749 [00:03<01:16, 46.66it/s, loss=2.29]Epoch 0:   5%|▍         | 170/3749 [00:03<01:16, 46.66it/s, loss=2.3] Epoch 0:   5%|▍         | 171/3749 [00:03<01:16, 46.72it/s, loss=2.3]Epoch 0:   5%|▍         | 172/3749 [00:03<01:16, 46.75it/s, loss=2.29]Epoch 0:   5%|▍         | 173/3749 [00:03<01:16, 46.81it/s, loss=2.29]Epoch 0:   5%|▍         | 174/3749 [00:03<01:16, 46.87it/s, loss=2.29]Epoch 0:   5%|▍         | 175/3749 [00:03<01:16, 46.89it/s, loss=2.29]Epoch 0:   5%|▍         | 175/3749 [00:03<01:16, 46.89it/s, loss=2.29]Epoch 0:   5%|▍         | 176/3749 [00:03<01:16, 46.91it/s, loss=2.29]Epoch 0:   5%|▍         | 177/3749 [00:03<01:16, 46.95it/s, loss=2.29]Epoch 0:   5%|▍         | 178/3749 [00:03<01:15, 47.02it/s, loss=2.29]Epoch 0:   5%|▍         | 179/3749 [00:03<01:15, 47.06it/s, loss=2.29]Epoch 0:   5%|▍         | 180/3749 [00:03<01:15, 47.07it/s, loss=2.3] Epoch 0:   5%|▍         | 181/3749 [00:03<01:15, 47.13it/s, loss=2.3]Epoch 0:   5%|▍         | 181/3749 [00:03<01:15, 47.13it/s, loss=2.3]Epoch 0:   5%|▍         | 182/3749 [00:03<01:15, 47.17it/s, loss=2.3]Epoch 0:   5%|▍         | 183/3749 [00:03<01:15, 47.23it/s, loss=2.3]Epoch 0:   5%|▍         | 184/3749 [00:03<01:15, 47.27it/s, loss=2.3]Epoch 0:   5%|▍         | 185/3749 [00:03<01:15, 47.29it/s, loss=2.3]Epoch 0:   5%|▍         | 186/3749 [00:03<01:15, 47.35it/s, loss=2.3]Epoch 0:   5%|▍         | 187/3749 [00:03<01:15, 47.40it/s, loss=2.3]Epoch 0:   5%|▍         | 187/3749 [00:03<01:15, 47.40it/s, loss=2.3]Epoch 0:   5%|▌         | 188/3749 [00:03<01:15, 47.44it/s, loss=2.3]Epoch 0:   5%|▌         | 189/3749 [00:03<01:14, 47.50it/s, loss=2.3]Epoch 0:   5%|▌         | 190/3749 [00:03<01:14, 47.54it/s, loss=2.3]Epoch 0:   5%|▌         | 191/3749 [00:04<01:14, 47.59it/s, loss=2.3]Epoch 0:   5%|▌         | 192/3749 [00:04<01:14, 47.65it/s, loss=2.3]Epoch 0:   5%|▌         | 193/3749 [00:04<01:14, 47.68it/s, loss=2.3]Epoch 0:   5%|▌         | 193/3749 [00:04<01:14, 47.68it/s, loss=2.31]Epoch 0:   5%|▌         | 194/3749 [00:04<01:14, 47.74it/s, loss=2.3] Epoch 0:   5%|▌         | 195/3749 [00:04<01:14, 47.78it/s, loss=2.3]Epoch 0:   5%|▌         | 196/3749 [00:04<01:14, 47.80it/s, loss=2.3]Epoch 0:   5%|▌         | 197/3749 [00:04<01:14, 47.84it/s, loss=2.3]Epoch 0:   5%|▌         | 198/3749 [00:04<01:14, 47.83it/s, loss=2.3]Epoch 0:   5%|▌         | 199/3749 [00:04<01:14, 47.88it/s, loss=2.3]Epoch 0:   5%|▌         | 199/3749 [00:04<01:14, 47.88it/s, loss=2.3]Epoch 0:   5%|▌         | 200/3749 [00:04<01:14, 47.91it/s, loss=2.3]Epoch 0:   5%|▌         | 201/3749 [00:04<01:14, 47.91it/s, loss=2.3]Epoch 0:   5%|▌         | 202/3749 [00:04<01:14, 47.92it/s, loss=2.3]Epoch 0:   5%|▌         | 203/3749 [00:04<01:13, 47.93it/s, loss=2.3]Epoch 0:   5%|▌         | 204/3749 [00:04<01:13, 47.95it/s, loss=2.3]Epoch 0:   5%|▌         | 205/3749 [00:04<01:13, 47.99it/s, loss=2.3]Epoch 0:   5%|▌         | 205/3749 [00:04<01:13, 47.99it/s, loss=2.29]Epoch 0:   5%|▌         | 206/3749 [00:04<01:13, 48.02it/s, loss=2.29]Epoch 0:   6%|▌         | 207/3749 [00:04<01:13, 48.07it/s, loss=2.3] Epoch 0:   6%|▌         | 208/3749 [00:04<01:13, 48.10it/s, loss=2.3]Epoch 0:   6%|▌         | 209/3749 [00:04<01:13, 48.15it/s, loss=2.3]Epoch 0:   6%|▌         | 210/3749 [00:04<01:13, 48.19it/s, loss=2.3]Epoch 0:   6%|▌         | 211/3749 [00:04<01:13, 48.22it/s, loss=2.3]Epoch 0:   6%|▌         | 211/3749 [00:04<01:13, 48.22it/s, loss=2.29]Epoch 0:   6%|▌         | 212/3749 [00:04<01:13, 48.27it/s, loss=2.29]Epoch 0:   6%|▌         | 213/3749 [00:04<01:13, 48.28it/s, loss=2.29]Epoch 0:   6%|▌         | 214/3749 [00:04<01:13, 48.30it/s, loss=2.29]Epoch 0:   6%|▌         | 215/3749 [00:04<01:13, 48.34it/s, loss=2.29]Epoch 0:   6%|▌         | 216/3749 [00:04<01:13, 48.35it/s, loss=2.29]Epoch 0:   6%|▌         | 217/3749 [00:04<01:12, 48.40it/s, loss=2.29]Epoch 0:   6%|▌         | 217/3749 [00:04<01:12, 48.40it/s, loss=2.29]Epoch 0:   6%|▌         | 218/3749 [00:04<01:12, 48.42it/s, loss=2.29]Epoch 0:   6%|▌         | 219/3749 [00:04<01:12, 48.45it/s, loss=2.29]Epoch 0:   6%|▌         | 220/3749 [00:04<01:12, 48.49it/s, loss=2.3] Epoch 0:   6%|▌         | 221/3749 [00:04<01:12, 48.52it/s, loss=2.29]Epoch 0:   6%|▌         | 222/3749 [00:04<01:12, 48.57it/s, loss=2.3] Epoch 0:   6%|▌         | 223/3749 [00:04<01:12, 48.58it/s, loss=2.3]Epoch 0:   6%|▌         | 223/3749 [00:04<01:12, 48.58it/s, loss=2.29]Epoch 0:   6%|▌         | 224/3749 [00:04<01:12, 48.61it/s, loss=2.3] Epoch 0:   6%|▌         | 225/3749 [00:04<01:12, 48.66it/s, loss=2.29]Epoch 0:   6%|▌         | 226/3749 [00:04<01:12, 48.68it/s, loss=2.3] Epoch 0:   6%|▌         | 227/3749 [00:04<01:12, 48.72it/s, loss=2.3]Epoch 0:   6%|▌         | 228/3749 [00:04<01:12, 48.74it/s, loss=2.3]Epoch 0:   6%|▌         | 229/3749 [00:04<01:12, 48.78it/s, loss=2.3]Epoch 0:   6%|▌         | 229/3749 [00:04<01:12, 48.77it/s, loss=2.3]Epoch 0:   6%|▌         | 230/3749 [00:04<01:12, 48.82it/s, loss=2.29]Epoch 0:   6%|▌         | 231/3749 [00:04<01:12, 48.83it/s, loss=2.3] Epoch 0:   6%|▌         | 232/3749 [00:04<01:11, 48.88it/s, loss=2.3]Epoch 0:   6%|▌         | 233/3749 [00:04<01:11, 48.90it/s, loss=2.3]Epoch 0:   6%|▌         | 234/3749 [00:04<01:11, 48.92it/s, loss=2.3]Epoch 0:   6%|▋         | 235/3749 [00:04<01:11, 48.97it/s, loss=2.3]Epoch 0:   6%|▋         | 235/3749 [00:04<01:11, 48.97it/s, loss=2.3]Epoch 0:   6%|▋         | 236/3749 [00:04<01:11, 48.99it/s, loss=2.3]Epoch 0:   6%|▋         | 237/3749 [00:04<01:11, 49.02it/s, loss=2.3]Epoch 0:   6%|▋         | 238/3749 [00:04<01:11, 49.06it/s, loss=2.3]Epoch 0:   6%|▋         | 239/3749 [00:04<01:11, 49.08it/s, loss=2.3]Epoch 0:   6%|▋         | 240/3749 [00:04<01:11, 49.12it/s, loss=2.3]Epoch 0:   6%|▋         | 241/3749 [00:04<01:11, 49.15it/s, loss=2.3]Epoch 0:   6%|▋         | 241/3749 [00:04<01:11, 49.15it/s, loss=2.3]Epoch 0:   6%|▋         | 242/3749 [00:04<01:11, 49.18it/s, loss=2.3]Epoch 0:   6%|▋         | 243/3749 [00:04<01:11, 49.22it/s, loss=2.3]Epoch 0:   7%|▋         | 244/3749 [00:04<01:11, 49.23it/s, loss=2.3]Epoch 0:   7%|▋         | 245/3749 [00:04<01:11, 49.27it/s, loss=2.3]Epoch 0:   7%|▋         | 246/3749 [00:04<01:11, 49.27it/s, loss=2.3]Epoch 0:   7%|▋         | 247/3749 [00:05<01:11, 49.31it/s, loss=2.3]Epoch 0:   7%|▋         | 247/3749 [00:05<01:11, 49.31it/s, loss=2.3]Epoch 0:   7%|▋         | 248/3749 [00:05<01:10, 49.35it/s, loss=2.29]Epoch 0:   7%|▋         | 249/3749 [00:05<01:10, 49.37it/s, loss=2.3] Epoch 0:   7%|▋         | 250/3749 [00:05<01:10, 49.41it/s, loss=2.3]Epoch 0:   7%|▋         | 251/3749 [00:05<01:10, 49.43it/s, loss=2.3]Epoch 0:   7%|▋         | 252/3749 [00:05<01:10, 49.41it/s, loss=2.3]Epoch 0:   7%|▋         | 253/3749 [00:05<01:10, 49.39it/s, loss=2.3]Epoch 0:   7%|▋         | 253/3749 [00:05<01:10, 49.38it/s, loss=2.3]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Files already downloaded and verified
Testing: 0it [00:00, ?it/s]Exception ignored in: <function _releaseLock at 0x7fab764b7700>
Traceback (most recent call last):
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/logging/__init__.py", line 227, in _releaseLock
    def _releaseLock():
KeyboardInterrupt: 
Error executing job with overrides: ['wandb=null', 'experiment=gtu-lra-cifar', 'trainer.gpus=1', 'loader.batch_size=16', 'loader.num_workers=4', 'scheduler.num_warmup_steps=30000', 'optimizer.lr=0.007', 'optimizer.weight_decay=0.001', 'model.n_layers=2', 'model.d_model=32', 'model.norm=batch', 'model.prenorm=True', 'train.seed=2222', 'model.gtu_rpe_dim=16', 'model.expand_ratio_gtu=2', 'model.expand_ratio_glu=3', 'model.gtu_use_decay=True', 'model.gtu_gamma=0.7', 'model.dropout=0.1']
Traceback (most recent call last):
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/queue.py", line 178, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/qinzhen/code/lra/train.py", line 585, in <module>
    main()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data/qinzhen/code/lra/train.py", line 581, in main
    train(config)
  File "/data/qinzhen/code/lra/train.py", line 518, in train
    trainer.test(model)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 911, in test
    return self._call_and_handle_interrupt(self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 954, in _test_impl
    results = self._run(model, ckpt_path=self.tested_ckpt_path)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1275, in _dispatch
    self.training_type_plugin.start_evaluating(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 206, in start_evaluating
    self._results = trainer.run_stage()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1286, in run_stage
    return self._run_evaluate()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1334, in _run_evaluate
    eval_loop_results = self._evaluation_loop.run()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 110, in advance
    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 140, in run
    self.on_run_start(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 86, in on_run_start
    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py", line 121, in _update_dataloader_iter
    dataloader_iter = enumerate(data_fetcher, batch_idx)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 199, in __iter__
    self.prefetching(self.prefetch_batches)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 258, in prefetching
    self._fetch_next_batch()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 300, in _fetch_next_batch
    batch = next(self.dataloader_iter)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 25371) exited unexpectedly
