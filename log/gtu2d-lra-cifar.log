CONFIG
├── train
│   └── seed: 2222                                                                    
│       interval: step                                                                
│       monitor: val/accuracy                                                         
│       mode: max                                                                     
│       ema: 0.0                                                                      
│       test: true                                                                    
│       debug: false                                                                  
│       ignore_warnings: false                                                        
│       state:                                                                        
│         mode: null                                                                  
│         chunk_len: null                                                             
│         overlap_len: null                                                           
│         n_context: 0                                                                
│         n_context_eval: 0                                                           
│       sweep: null                                                                   
│       group: null                                                                   
│       benchmark_step: false                                                         
│       benchmark_step_k: 1                                                           
│       benchmark_step_T: 1                                                           
│       checkpoint_path: null                                                         
│       visualizer: filters                                                           
│       disable_dataset: false                                                        
│                                                                                     
├── wandb
│   └── None                                                                          
├── trainer
│   └── gpus: 1                                                                       
│       accumulate_grad_batches: 1                                                    
│       max_epochs: 200                                                               
│       gradient_clip_val: 0.0                                                        
│       log_every_n_steps: 10                                                         
│       limit_train_batches: 1.0                                                      
│       limit_val_batches: 1.0                                                        
│       weights_summary: top                                                          
│       progress_bar_refresh_rate: 1                                                  
│       track_grad_norm: -1                                                           
│       resume_from_checkpoint: null                                                  
│                                                                                     
├── loader
│   └── batch_size: 16                                                                
│       num_workers: 4                                                                
│       pin_memory: true                                                              
│       drop_last: true                                                               
│       train_resolution: 1                                                           
│       eval_resolutions:                                                             
│       - 1                                                                           
│                                                                                     
├── dataset
│   └── _name_: cifar                                                                 
│       permute: null                                                                 
│       grayscale: true                                                               
│       tokenize: false                                                               
│       augment: false                                                                
│       cutout: false                                                                 
│       random_erasing: false                                                         
│       val_split: 0.1                                                                
│       seed: 42                                                                      
│                                                                                     
├── task
│   └── _name_: base                                                                  
│       loss: cross_entropy                                                           
│       metrics:                                                                      
│       - accuracy                                                                    
│       torchmetrics: null                                                            
│                                                                                     
├── optimizer
│   └── _name_: adamw                                                                 
│       lr: 0.007                                                                     
│       weight_decay: 0.001                                                           
│                                                                                     
├── scheduler
│   └── _name_: cosine_warmup                                                         
│       num_warmup_steps: 30000                                                       
│       num_training_steps: 50000                                                     
│                                                                                     
├── encoder
│   └── linear                                                                        
├── decoder
│   └── _name_: sequence                                                              
│       mode: pool                                                                    
│                                                                                     
├── model
│   └── _name_: model                                                                 
│       layer:                                                                        
│       - _name_: gtu2d                                                               
│         n_heads: 1                                                                  
│         expand_ratio: 2                                                             
│         use_decay: true                                                             
│         gamma: 0.7                                                                  
│         rpe_embedding: 16                                                           
│         rpe_layers: 2                                                               
│       - _name_: glu                                                                 
│         act_fun: swish                                                              
│         glu_expand_ratio: 3                                                         
│       rpe_layers: 2                                                                 
│       gtu_head: 1                                                                   
│       gtu_use_decay: true                                                           
│       gtu_gamma: 0.7                                                                
│       gtu_rpe_dim: 16                                                               
│       expand_ratio_gtu: 2                                                           
│       expand_ratio_glu: 3                                                           
│       flash_s: 0                                                                    
│       flash_max_position_embed: 0                                                   
│       flash_linear_s: 0                                                             
│       flash_linear_max_position_embeddings: 0                                       
│       lg_local_heads: 0                                                             
│       lg_linear_heads: 0                                                            
│       lg_local_chunk_size: 0                                                        
│       ls_attn_heads: 0                                                              
│       ls_attn_window_size: 0                                                        
│       ls_attn_max_seq_len: 0                                                        
│       performer_heads: 0                                                            
│       performer_approx_attn_dim: 0                                                  
│       use_softmax: true                                                             
│       act_fun: 1+elu                                                                
│       cosformer_heads: 0                                                            
│       cosformer_max_length: 0                                                       
│       linformer_max_seq_len: 0                                                      
│       residual: R                                                                   
│       dropout: 0.1                                                                  
│       encoder:                                                                      
│         _name_: position                                                            
│         dropout: 0.1                                                                
│       n_layers: 2                                                                   
│       d_model: 32                                                                   
│       prenorm: true                                                                 
│       norm: batch                                                                   
│                                                                                     
└── callbacks
    └── learning_rate_monitor:                                                        
          logging_interval: step                                                      
        timer:                                                                        
          step: true                                                                  
          inter_step: false                                                           
          epoch: true                                                                 
          val: true                                                                   
        params:                                                                       
          total: true                                                                 
          trainable: true                                                             
          fixed: true                                                                 
        model_checkpoint:                                                             
          monitor: val/accuracy                                                       
          mode: max                                                                   
          save_top_k: 1                                                               
          save_last: true                                                             
          dirpath: checkpoints/                                                       
          filename: val/accuracy                                                      
          auto_insert_metric_name: false                                              
          verbose: true                                                               
                                                                                      
Global seed set to 2222
[2022-12-21 17:41:26,270][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2022-12-21 17:41:26,272][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2022-12-21 17:41:26,273][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Files already downloaded and verified
[2022-12-21 17:41:28,100][print_config][INFO] - activation: silu
[2022-12-21 17:41:28,106][print_config][INFO] - activation: none
[2022-12-21 17:41:28,107][print_config][INFO] - activation: none
swish
None
act_fun swish
dropout 0.1
final None
[2022-12-21 17:41:28,112][print_config][INFO] - activation: silu
[2022-12-21 17:41:28,115][print_config][INFO] - activation: none
[2022-12-21 17:41:28,116][print_config][INFO] - activation: none
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
swish
None
act_fun swish
dropout 0.1
final None
SequenceModel(
  (drop): Identity()
  (layers): ModuleList(
    (0): SequenceResidualBlock(
      (layer): Gtu2d(
        (dropout): Dropout(p=0.1, inplace=False)
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep1): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (layers): ModuleList(
              (0): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
              (1): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
            )
            (out): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
        (toep2): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (layers): ModuleList(
              (0): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
              (1): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
            )
            (out): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): SequenceResidualBlock(
      (layer): Gtu2d(
        (dropout): Dropout(p=0.1, inplace=False)
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep1): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (layers): ModuleList(
              (0): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
              (1): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
            )
            (out): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
        (toep2): Tno(
          (rpe): Rpe(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (layers): ModuleList(
              (0): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
              (1): Sequential(
                (0): ReLU(inplace=True)
                (1): Linear(in_features=16, out_features=16, bias=True)
              )
            )
            (out): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (3): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (norm): Normalization(
    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
[2022-12-21 17:41:32,887][__main__][INFO] - Optimizer group 0 | 82 tensors

  | Name    | Type            | Params
--------------------------------------------
0 | model   | SequenceModel   | 38.8 K
1 | encoder | Sequential      | 64    
2 | decoder | SequenceDecoder | 330   
--------------------------------------------
39.0 K    Trainable params
256       Non-trainable params
39.2 K    Total params
0.157     Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/4 [00:00<?, ?it/s]Error executing job with overrides: ['wandb=null', 'experiment=gtu2d-lra-cifar', 'trainer.gpus=1', 'loader.batch_size=16', 'loader.num_workers=4', 'scheduler.num_warmup_steps=30000', 'optimizer.lr=0.007', 'optimizer.weight_decay=0.001', 'model.n_layers=2', 'model.d_model=32', 'model.norm=batch', 'model.prenorm=True', 'train.seed=2222', 'model.gtu_rpe_dim=16', 'model.expand_ratio_gtu=2', 'model.expand_ratio_glu=3', 'model.gtu_use_decay=True', 'model.gtu_gamma=0.7', 'model.dropout=0.1']
Traceback (most recent call last):
  File "/data/qinzhen/code/lra/train.py", line 585, in <module>
    main()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data/qinzhen/code/lra/train.py", line 581, in main
    train(config)
  File "/data/qinzhen/code/lra/train.py", line 516, in train
    trainer.fit(model)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1311, in _run_train
    self._run_sanity_check(self.lightning_module)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1375, in _run_sanity_check
    self._evaluation_loop.run()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 110, in advance
    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 140, in run
    self.on_run_start(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 86, in on_run_start
    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py", line 121, in _update_dataloader_iter
    dataloader_iter = enumerate(data_fetcher, batch_idx)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 197, in __iter__
    self.dataloader_iter = iter(self.dataloader)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 359, in __iter__
    return self._get_iterator()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 918, in __init__
    w.start()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/popen_fork.py", line 70, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
                                                              