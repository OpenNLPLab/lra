CONFIG
├── train
│   └── seed: 2222                                                                             
│       interval: step                                                                         
│       monitor: val/accuracy                                                                  
│       mode: max                                                                              
│       ema: 0.0                                                                               
│       test: true                                                                             
│       debug: false                                                                           
│       ignore_warnings: false                                                                 
│       state:                                                                                 
│         mode: null                                                                           
│         chunk_len: null                                                                      
│         overlap_len: null                                                                    
│         n_context: 0                                                                         
│         n_context_eval: 0                                                                    
│       sweep: null                                                                            
│       group: null                                                                            
│       benchmark_step: false                                                                  
│       benchmark_step_k: 1                                                                    
│       benchmark_step_T: 1                                                                    
│       checkpoint_path: null                                                                  
│       visualizer: filters                                                                    
│       disable_dataset: false                                                                 
│                                                                                              
├── wandb
│   └── None                                                                                   
├── trainer
│   └── gpus: 1                                                                                
│       accumulate_grad_batches: 1                                                             
│       max_epochs: 200                                                                        
│       gradient_clip_val: 0.0                                                                 
│       log_every_n_steps: 10                                                                  
│       limit_train_batches: 1.0                                                               
│       limit_val_batches: 1.0                                                                 
│       weights_summary: top                                                                   
│       progress_bar_refresh_rate: 1                                                           
│       track_grad_norm: -1                                                                    
│       resume_from_checkpoint: null                                                           
│                                                                                              
├── loader
│   └── batch_size: 16                                                                         
│       num_workers: 4                                                                         
│       pin_memory: true                                                                       
│       drop_last: true                                                                        
│       train_resolution: 1                                                                    
│       eval_resolutions:                                                                      
│       - 1                                                                                    
│                                                                                              
├── dataset
│   └── _name_: cifar                                                                          
│       permute: null                                                                          
│       grayscale: true                                                                        
│       tokenize: false                                                                        
│       augment: false                                                                         
│       cutout: false                                                                          
│       random_erasing: false                                                                  
│       val_split: 0.1                                                                         
│       seed: 42                                                                               
│                                                                                              
├── task
│   └── _name_: base                                                                           
│       loss: cross_entropy                                                                    
│       metrics:                                                                               
│       - accuracy                                                                             
│       torchmetrics: null                                                                     
│                                                                                              
├── optimizer
│   └── _name_: adamw                                                                          
│       lr: 0.007                                                                              
│       weight_decay: 0.001                                                                    
│                                                                                              
├── scheduler
│   └── _name_: cosine_warmup                                                                  
│       num_warmup_steps: 30000                                                                
│       num_training_steps: 50000                                                              
│                                                                                              
├── encoder
│   └── linear                                                                                 
├── decoder
│   └── _name_: sequence                                                                       
│       mode: pool                                                                             
│                                                                                              
├── model
│   └── _name_: model                                                                          
│       layer:                                                                                 
│       - _name_: tno                                                                          
│         n_heads: 1                                                                           
│         tno_max_l: 1024                                                                      
│         tno_type: 4                                                                          
│         tno_expand_ratio: 2                                                                  
│         use_decay: true                                                                      
│         gamma: 0.7                                                                           
│         dpb_embedding: 16                                                                    
│         dpb_type: 1                                                                          
│         dpb_layers: 0                                                                        
│       - _name_: glu                                                                          
│         act_fun: swish                                                                       
│         glu_expand_ratio: 3                                                                  
│       tno_head: 1                                                                            
│       tno_max_l: 1024                                                                        
│       tno_type: 4                                                                            
│       tno_use_decay: true                                                                    
│       tno_gamma: 0.7                                                                         
│       tno_dpb_dim: 16                                                                        
│       expand_ratio_tno: 2                                                                    
│       expand_ratio_glu: 3                                                                    
│       dpb_type: 1                                                                            
│       dpb_layers: 0                                                                          
│       flash_s: 0                                                                             
│       flash_max_position_embed: 0                                                            
│       flash_linear_s: 0                                                                      
│       flash_linear_max_position_embeddings: 0                                                
│       lg_local_heads: 0                                                                      
│       lg_linear_heads: 0                                                                     
│       lg_local_chunk_size: 0                                                                 
│       ls_attn_heads: 0                                                                       
│       ls_attn_window_size: 0                                                                 
│       ls_attn_max_seq_len: 0                                                                 
│       performer_heads: 0                                                                     
│       performer_approx_attn_dim: 0                                                           
│       use_softmax: true                                                                      
│       act_fun: 1+elu                                                                         
│       cosformer_heads: 0                                                                     
│       cosformer_max_length: 0                                                                
│       linformer_max_seq_len: 0                                                               
│       residual: R                                                                            
│       dropout: 0.1                                                                           
│       encoder:                                                                               
│         _name_: position                                                                     
│         dropout: 0.1                                                                         
│       n_layers: 2                                                                            
│       d_model: 32                                                                            
│       prenorm: true                                                                          
│       norm: batch                                                                            
│                                                                                              
└── callbacks
    └── learning_rate_monitor:                                                                 
          logging_interval: step                                                               
        timer:                                                                                 
          step: true                                                                           
          inter_step: false                                                                    
          epoch: true                                                                          
          val: true                                                                            
        params:                                                                                
          total: true                                                                          
          trainable: true                                                                      
          fixed: true                                                                          
        model_checkpoint:                                                                      
          monitor: val/accuracy                                                                
          mode: max                                                                            
          save_top_k: 1                                                                        
          save_last: true                                                                      
          dirpath: checkpoints/                                                                
          filename: val/accuracy                                                               
          auto_insert_metric_name: false                                                       
          verbose: true                                                                        
                                                                                               
Global seed set to 2222
[2022-12-21 17:05:42,997][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2022-12-21 17:05:42,999][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2022-12-21 17:05:42,999][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Files already downloaded and verified
self.expand_ratio 2
self.resi_param False
silu
act_fun silu
causal False
none
self.num_heads 1
self.max_l 1024
self.use_exp False
self.use_neg_exp False
self.use_decay True
self.use_multi_decay False
self.dpb_embedding 16
self.dpb_act relu
self.dpb_use_pad True
self.normalize False
self.par_type 1
self.dpb_type 1
self.residual False
self.l 1
self.transform_type 1
self.gamma 0.7
bias True
tno_type 4
dpb_layers 0
use_norm False
norm_type layernorm
self.token_shift_type -1
swish
None
act_fun swish
dropout 0.1
final None
self.expand_ratio 2
self.resi_param False
silu
act_fun silu
causal False
none
self.num_heads 1
self.max_l 1024
self.use_exp False
self.use_neg_exp False
self.use_decay True
self.use_multi_decay False
self.dpb_embedding 16
self.dpb_act relu
self.dpb_use_pad True
self.normalize False
self.par_type 1
self.dpb_type 1
self.residual False
self.l 1
self.transform_type 1
self.gamma 0.7
bias True
tno_type 4
dpb_layers 0
use_norm False
norm_type layernorm
self.token_shift_type -1
swish
None
act_fun swish
dropout 0.1
final None
SequenceModel(
  (drop): Identity()
  (layers): ModuleList(
    (0): SequenceResidualBlock(
      (layer): TNO(
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep): DynamicToepliztMultiheadV4(
          (dpb): DynamicPosBiasV4(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (pos1): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos2): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos3): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): SequenceResidualBlock(
      (layer): TNO(
        (v_proj): Linear(in_features=32, out_features=64, bias=True)
        (u_proj): Linear(in_features=32, out_features=64, bias=True)
        (o): Linear(in_features=64, out_features=32, bias=True)
        (toep): DynamicToepliztMultiheadV4(
          (dpb): DynamicPosBiasV4(
            (pos_proj): Linear(in_features=1, out_features=16, bias=True)
            (pos1): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos2): Sequential(
              (0): ReLU(inplace=True)
              (1): Linear(in_features=16, out_features=16, bias=True)
            )
            (pos3): Sequential(
              (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=64, bias=True)
            )
          )
        )
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
    (3): SequenceResidualBlock(
      (layer): GLU(
        (l1): Linear(in_features=32, out_features=96, bias=True)
        (l2): Linear(in_features=32, out_features=96, bias=True)
        (l3): Linear(in_features=96, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (residual): Residual()
      (norm): Normalization(
        (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (norm): Normalization(
    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
[2022-12-21 17:05:48,635][__main__][INFO] - Optimizer group 0 | 60 tensors

  | Name    | Type            | Params
--------------------------------------------
0 | model   | SequenceModel   | 35.3 K
1 | encoder | Sequential      | 64    
2 | decoder | SequenceDecoder | 330   
--------------------------------------------
35.6 K    Trainable params
128       Non-trainable params
35.7 K    Total params
0.143     Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/4 [00:00<?, ?it/s]Validation sanity check:  25%|██▌       | 1/4 [00:00<00:01,  2.68it/s]Validation sanity check:  75%|███████▌  | 3/4 [00:00<00:00,  4.95it/s]                                                                      Global seed set to 2222
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/3749 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3749 [00:00<?, ?it/s] Error executing job with overrides: ['wandb=null', 'experiment=tno-lra-cifar', 'trainer.gpus=1', 'loader.batch_size=16', 'loader.num_workers=4', 'scheduler.num_warmup_steps=30000', 'optimizer.lr=0.007', 'optimizer.weight_decay=0.001', 'model.n_layers=2', 'model.d_model=32', 'model.norm=batch', 'model.prenorm=True', 'train.seed=2222', 'model.tno_dpb_dim=16', 'model.expand_ratio_tno=2', 'model.expand_ratio_glu=3', 'model.tno_use_decay=True', 'model.tno_gamma=0.7', 'model.dropout=0.1', 'model.dpb_type=1', 'model.dpb_layers=0']
Traceback (most recent call last):
  File "/data/qinzhen/code/lra/train.py", line 585, in <module>
    main()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data/qinzhen/code/lra/train.py", line 581, in main
    train(config)
  File "/data/qinzhen/code/lra/train.py", line 516, in train
    trainer.fit(model)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 140, in run
    self.on_run_start(*args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 141, in on_run_start
    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_idx + 1)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py", line 121, in _update_dataloader_iter
    dataloader_iter = enumerate(data_fetcher, batch_idx)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 198, in __iter__
    self._apply_patch()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 133, in _apply_patch
    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 181, in loader_iters
    loader_iters = self.dataloader_iter.loader_iters
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py", line 537, in loader_iters
    self._loader_iters = self.create_loader_iters(self.loaders)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py", line 577, in create_loader_iters
    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py", line 96, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 359, in __iter__
    return self._get_iterator()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 918, in __init__
    w.start()
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/SENSETIME/qinzhen/anaconda3/envs/lra/lib/python3.8/multiprocessing/popen_fork.py", line 70, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
